"""
Intent Classification Evaluation Module
Precision, Recall, F1 Score metrikleri ile deÄŸerlendirme yapar.
"""

import os
import random
from typing import Dict, List, Tuple
from collections import defaultdict
from intent_classifier import IntentClassifier


class IntentEvaluator:
    """Intent Classification deÄŸerlendirme sÄ±nÄ±fÄ±"""
    
    def __init__(self, classifier: IntentClassifier, test_ratio: float = 0.2):
        """
        Args:
            classifier: EÄŸitilmiÅŸ IntentClassifier
            test_ratio: Test seti oranÄ± (varsayÄ±lan %20)
        """
        self.classifier = classifier
        self.test_ratio = test_ratio
        self.test_data: List[Tuple[str, str]] = []
        self.predictions: List[Tuple[str, str, str]] = []  # (text, actual, predicted)
        
    def prepare_test_data(self, test_file: str = "test_intents.txt"):
        """Test verisini ayrÄ± dosyadan yÃ¼kler"""
        
        # Ã–nce ayrÄ± test dosyasÄ±nÄ± dene
        if os.path.exists(test_file):
            self._load_from_file(test_file)
            print(f"âœ… AyrÄ± test dosyasÄ±ndan {len(self.test_data)} Ã¶rnek yÃ¼klendi: {test_file}")
        else:
            # AyrÄ± dosya yoksa eÄŸitim verisinden ayÄ±r
            print(f"âš ï¸ {test_file} bulunamadÄ±, eÄŸitim verisinden ayÄ±rÄ±lÄ±yor...")
            self._split_from_training("intents.txt")
    
    def _load_from_file(self, filepath: str):
        """Dosyadan test verisi yÃ¼kler"""
        self.test_data = []
        
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                if '|' in line:
                    parts = line.split('|', 1)
                    if len(parts) == 2:
                        intent, text = parts
                        self.test_data.append((intent.strip().lower(), text.strip()))
    
    def _split_from_training(self, data_file: str):
        """EÄŸitim verisinden test seti ayÄ±rÄ±r (yedek yÃ¶ntem)"""
        all_data: Dict[str, List[str]] = defaultdict(list)
        
        if not os.path.exists(data_file):
            print(f"âŒ {data_file} bulunamadÄ±!")
            return
        
        with open(data_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                if '|' in line:
                    parts = line.split('|', 1)
                    if len(parts) == 2:
                        intent, text = parts
                        all_data[intent.strip().lower()].append(text.strip())
        
        # Her kategoriden test_ratio kadar Ã¶rnek ayÄ±r
        self.test_data = []
        for intent, texts in all_data.items():
            random.seed(42)  # Tekrarlanabilirlik iÃ§in
            n_test = max(1, int(len(texts) * self.test_ratio))
            test_samples = random.sample(texts, min(n_test, len(texts)))
            for text in test_samples:
                self.test_data.append((intent, text))
        
        print(f"âœ… {len(self.test_data)} test Ã¶rneÄŸi hazÄ±rlandÄ±.")
    
    def evaluate(self) -> Dict:
        """DeÄŸerlendirme yapar ve metrikleri hesaplar"""
        if not self.test_data:
            self.prepare_test_data()
        
        self.predictions = []
        
        # Her test Ã¶rneÄŸi iÃ§in tahmin yap
        for actual_intent, text in self.test_data:
            predicted_intent, score, _ = self.classifier.classify(text)
            self.predictions.append((text, actual_intent, predicted_intent))
        
        # Metrikleri hesapla
        return self._calculate_metrics()
    
    def _calculate_metrics(self) -> Dict:
        """Precision, Recall, F1 Score hesaplar"""
        # TÃ¼m intent'leri topla
        all_intents = set()
        for _, actual, predicted in self.predictions:
            all_intents.add(actual)
            all_intents.add(predicted)
        
        # Her intent iÃ§in TP, FP, FN hesapla
        metrics = {}
        
        for intent in all_intents:
            tp = 0  # True Positive
            fp = 0  # False Positive
            fn = 0  # False Negative
            
            for _, actual, predicted in self.predictions:
                if actual == intent and predicted == intent:
                    tp += 1
                elif actual != intent and predicted == intent:
                    fp += 1
                elif actual == intent and predicted != intent:
                    fn += 1
            
            # Precision = TP / (TP + FP)
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            
            # Recall = TP / (TP + FN)
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            
            # F1 = 2 * (Precision * Recall) / (Precision + Recall)
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
            
            metrics[intent] = {
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'support': tp + fn  # GerÃ§ek Ã¶rneklerin sayÄ±sÄ±
            }
        
        # Macro average hesapla
        macro_precision = sum(m['precision'] for m in metrics.values()) / len(metrics)
        macro_recall = sum(m['recall'] for m in metrics.values()) / len(metrics)
        macro_f1 = sum(m['f1_score'] for m in metrics.values()) / len(metrics)
        
        # Accuracy hesapla
        correct = sum(1 for _, actual, predicted in self.predictions if actual == predicted)
        accuracy = correct / len(self.predictions) if self.predictions else 0.0
        
        return {
            'per_class': metrics,
            'macro_avg': {
                'precision': macro_precision,
                'recall': macro_recall,
                'f1_score': macro_f1
            },
            'accuracy': accuracy,
            'total_samples': len(self.predictions)
        }
    
    def get_confusion_matrix(self) -> Dict[str, Dict[str, int]]:
        """Confusion matrix oluÅŸturur"""
        if not self.predictions:
            self.evaluate()
        
        # TÃ¼m intent'leri al
        all_intents = sorted(set(
            [p[1] for p in self.predictions] + [p[2] for p in self.predictions]
        ))
        
        # Confusion matrix
        matrix: Dict[str, Dict[str, int]] = {
            intent: {i: 0 for i in all_intents} for intent in all_intents
        }
        
        for _, actual, predicted in self.predictions:
            matrix[actual][predicted] += 1
        
        return matrix
    
    def print_report(self):
        """DetaylÄ± deÄŸerlendirme raporu yazdÄ±rÄ±r"""
        results = self.evaluate()
        
        print("\n" + "="*70)
        print("ğŸ“Š INTENT CLASSIFICATION DEÄERLENDÄ°RME RAPORU")
        print("="*70)
        
        print(f"\nğŸ“ˆ Genel Metrikler:")
        print(f"   â€¢ Accuracy: {results['accuracy']:.2%}")
        print(f"   â€¢ Toplam Ã–rnek: {results['total_samples']}")
        
        print(f"\nğŸ“Š Macro Average:")
        print(f"   â€¢ Precision: {results['macro_avg']['precision']:.2%}")
        print(f"   â€¢ Recall: {results['macro_avg']['recall']:.2%}")
        print(f"   â€¢ F1 Score: {results['macro_avg']['f1_score']:.2%}")
        
        print("\n" + "-"*70)
        print(f"{'Kategori':<20} {'Precision':>12} {'Recall':>12} {'F1 Score':>12} {'Destek':>10}")
        print("-"*70)
        
        for intent, metrics in sorted(results['per_class'].items()):
            print(f"{intent:<20} {metrics['precision']:>11.2%} {metrics['recall']:>11.2%} {metrics['f1_score']:>11.2%} {metrics['support']:>10}")
        
        print("-"*70)
        
        # Confusion matrix
        print("\nğŸ“‹ Confusion Matrix (satÄ±rlar: gerÃ§ek, sÃ¼tunlar: tahmin):")
        matrix = self.get_confusion_matrix()
        intents = sorted(matrix.keys())
        
        # Header
        print(f"\n{'':>15}", end="")
        for intent in intents:
            print(f"{intent[:8]:>10}", end="")
        print()
        
        # Rows
        for actual in intents:
            print(f"{actual[:14]:<15}", end="")
            for predicted in intents:
                count = matrix[actual][predicted]
                if count > 0:
                    print(f"{count:>10}", end="")
                else:
                    print(f"{'Â·':>10}", end="")
            print()
        
        print("\n" + "="*70)
    
    def get_misclassified(self, limit: int = 10) -> List[Tuple[str, str, str]]:
        """YanlÄ±ÅŸ sÄ±nÄ±flandÄ±rÄ±lan Ã¶rnekleri dÃ¶ndÃ¼rÃ¼r"""
        if not self.predictions:
            self.evaluate()
        
        misclassified = [
            (text, actual, predicted) 
            for text, actual, predicted in self.predictions 
            if actual != predicted
        ]
        
        return misclassified[:limit]
    
    def save_report(self, filename: str = "evaluation_report.txt"):
        """DeÄŸerlendirme raporunu dosyaya kaydeder"""
        import io
        import sys
        
        # Ã‡Ä±ktÄ±yÄ± yakala
        old_stdout = sys.stdout
        sys.stdout = buffer = io.StringIO()
        
        self.print_report()
        
        # YanlÄ±ÅŸ sÄ±nÄ±flandÄ±rmalarÄ± ekle
        print("\nğŸ“› YANLIÅ SINIFLANDIRILAN Ã–RNEKLER:")
        print("-"*70)
        for text, actual, predicted in self.get_misclassified(20):
            print(f"Metin: {text[:50]}...")
            print(f"   GerÃ§ek: {actual} â†’ Tahmin: {predicted}")
            print()
        
        report = buffer.getvalue()
        sys.stdout = old_stdout
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ… Rapor kaydedildi: {filename}")
        return report


# Ana Ã§alÄ±ÅŸtÄ±rma
if __name__ == "__main__":
    print("ğŸ”„ Intent Classifier yÃ¼kleniyor...")
    classifier = IntentClassifier()
    
    print("ğŸ“Š DeÄŸerlendirme baÅŸlÄ±yor...")
    evaluator = IntentEvaluator(classifier, test_ratio=0.2)
    evaluator.print_report()
    
    # Raporu kaydet
    evaluator.save_report("evaluation_report.txt")
    
    # YanlÄ±ÅŸ sÄ±nÄ±flandÄ±rmalarÄ± gÃ¶ster
    print("\nğŸ“› Ã–rnek yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rmalar:")
    for text, actual, predicted in evaluator.get_misclassified(5):
        print(f"  â€¢ '{text[:40]}...'")
        print(f"    GerÃ§ek: {actual} â†’ Tahmin: {predicted}")
